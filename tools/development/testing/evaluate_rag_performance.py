#!/usr/bin/env python3
"""
RAGå¯¹æ¯”å­¦ä¹ æ¨¡å‹æ€§èƒ½è¯„ä¼°è„šæœ¬
å…¨é¢è¯„ä¼°è®­ç»ƒåçš„æ¨¡å‹åœ¨Aè‚¡èˆ†æƒ…åˆ†æä»»åŠ¡ä¸Šçš„æ€§èƒ½
"""

import os
import sys
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import List, Dict, Tuple
import json
import logging
from dataclasses import dataclass, asdict
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import yaml
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root / "services" / "decision_engine"))

from contrastive_learning_samples import A_STOCK_CONTRASTIVE_SAMPLES, SentimentLabel
from contrastive_rag_enhancer import ContrastiveEmbeddingModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç»“æ„"""
    model_name: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    auc_score: float
    avg_positive_similarity: float
    avg_negative_similarity: float
    similarity_gap: float
    category_results: Dict[str, Dict]
    confusion_matrix: List[List[int]]
    total_samples: int

class RAGPerformanceEvaluator:
    """RAGæ€§èƒ½è¯„ä¼°å™¨"""
    
    def __init__(self, model_path: str = None, config_path: str = None):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åŠ è½½é…ç½®
        if config_path and Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
        else:
            self.config = self._default_config()
        
        # åŠ è½½æ¨¡å‹
        self.model = self._load_model(model_path)
        
        # åŸºå‡†æ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
        self.baseline_model = SentenceTransformer("BAAI/bge-large-zh-v1.5")
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ: {model_path if model_path else 'åŸºç¡€æ¨¡å‹'}")
    
    def _default_config(self) -> Dict:
        """é»˜è®¤é…ç½®"""
        return {
            'evaluation': {
                'similarity_threshold': 0.7,
                'top_k': [1, 3, 5, 10],
                'metrics': ['accuracy', 'precision', 'recall', 'f1', 'auc']
            }
        }
    
    def _load_model(self, model_path: str = None) -> ContrastiveEmbeddingModel:
        """åŠ è½½è®­ç»ƒåçš„æ¨¡å‹"""
        if model_path and Path(model_path).exists():
            logger.info(f"åŠ è½½è®­ç»ƒæ¨¡å‹: {model_path}")
            
            # åˆå§‹åŒ–æ¨¡å‹
            model = ContrastiveEmbeddingModel()
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(model_path, map_location=self.device)  # TODO: Consider using weights_only=True for security
            model.load_state_dict(checkpoint['model_state_dict'])
            model.eval()
            
            return model.to(self.device)
        else:
            logger.info("ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œè¯„ä¼°")
            return ContrastiveEmbeddingModel().to(self.device)
    
    def prepare_test_data(self) -> List[Dict]:
        """å‡†å¤‡æµ‹è¯•æ•°æ®"""
        test_samples = []
        
        for category, samples in A_STOCK_CONTRASTIVE_SAMPLES.items():
            for sample in samples:
                test_samples.append({
                    'anchor': sample['anchor'],
                    'positive': sample.get('positive', sample['anchor']),
                    'hard_negative': sample['hard_negative'], 
                    'category': category,
                    'anchor_label': sample['anchor_label'],
                    'explanation': sample['explanation']
                })
        
        logger.info(f"å‡†å¤‡æµ‹è¯•æ ·æœ¬æ•°: {len(test_samples)}")
        return test_samples
    
    def evaluate_similarity_task(self, test_samples: List[Dict]) -> Dict:
        """è¯„ä¼°ç›¸ä¼¼åº¦ä»»åŠ¡æ€§èƒ½"""
        logger.info("è¯„ä¼°ç›¸ä¼¼åº¦åŒºåˆ†ä»»åŠ¡...")
        
        all_predictions = []
        all_labels = []
        all_pos_sims = []
        all_neg_sims = []
        category_results = {}
        
        # æŒ‰ç±»åˆ«è¯„ä¼°
        for category in set(sample['category'] for sample in test_samples):
            category_samples = [s for s in test_samples if s['category'] == category]
            category_predictions = []
            category_labels = []
            category_pos_sims = []
            category_neg_sims = []
            
            for sample in tqdm(category_samples, desc=f"è¯„ä¼°ç±»åˆ«: {category}"):
                # è·å–åµŒå…¥
                with torch.no_grad():
                    anchor_emb = self.model.encode([sample['anchor']])
                    positive_emb = self.model.encode([sample['positive']])
                    negative_emb = self.model.encode([sample['hard_negative']])
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                pos_sim = torch.cosine_similarity(anchor_emb, positive_emb, dim=1).item()
                neg_sim = torch.cosine_similarity(anchor_emb, negative_emb, dim=1).item()
                
                # é¢„æµ‹ï¼ˆæ­£æ ·æœ¬ç›¸ä¼¼åº¦åº”è¯¥é«˜äºè´Ÿæ ·æœ¬ï¼‰
                prediction = 1 if pos_sim > neg_sim else 0
                label = 1  # æ­£ç¡®ç­”æ¡ˆæ€»æ˜¯1
                
                category_predictions.append(prediction)
                category_labels.append(label)
                category_pos_sims.append(pos_sim)
                category_neg_sims.append(neg_sim)
                
                all_predictions.append(prediction)
                all_labels.append(label)
                all_pos_sims.append(pos_sim)
                all_neg_sims.append(neg_sim)
            
            # ç±»åˆ«ç»Ÿè®¡
            category_accuracy = accuracy_score(category_labels, category_predictions)
            category_results[category] = {
                'accuracy': category_accuracy,
                'avg_pos_sim': np.mean(category_pos_sims),
                'avg_neg_sim': np.mean(category_neg_sims),
                'similarity_gap': np.mean(category_pos_sims) - np.mean(category_neg_sims),
                'sample_count': len(category_samples)
            }
        
        # æ€»ä½“è¯„ä¼°æŒ‡æ ‡
        accuracy = accuracy_score(all_labels, all_predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            all_labels, all_predictions, average='binary'
        )
        
        # è®¡ç®—AUCï¼ˆä½¿ç”¨ç›¸ä¼¼åº¦å·®ä½œä¸ºscoreï¼‰
        similarity_diffs = np.array(all_pos_sims) - np.array(all_neg_sims)
        auc = roc_auc_score(all_labels, similarity_diffs)
        
        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_predictions)
        
        results = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_score': auc,
            'avg_positive_similarity': np.mean(all_pos_sims),
            'avg_negative_similarity': np.mean(all_neg_sims),
            'similarity_gap': np.mean(all_pos_sims) - np.mean(all_neg_sims),
            'category_results': category_results,
            'confusion_matrix': cm.tolist(),
            'total_samples': len(test_samples)
        }
        
        return results
    
    def evaluate_baseline_comparison(self, test_samples: List[Dict]) -> Dict:
        """ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”è¯„ä¼°"""
        logger.info("ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
        
        trained_results = []
        baseline_results = []
        
        for sample in tqdm(test_samples, desc="åŸºçº¿å¯¹æ¯”"):
            # è®­ç»ƒæ¨¡å‹ç»“æœ
            with torch.no_grad():
                anchor_emb_trained = self.model.encode([sample['anchor']])
                positive_emb_trained = self.model.encode([sample['positive']])
                negative_emb_trained = self.model.encode([sample['hard_negative']])
            
            pos_sim_trained = torch.cosine_similarity(anchor_emb_trained, positive_emb_trained, dim=1).item()
            neg_sim_trained = torch.cosine_similarity(anchor_emb_trained, negative_emb_trained, dim=1).item()
            
            # åŸºçº¿æ¨¡å‹ç»“æœ
            anchor_emb_baseline = self.baseline_model.encode([sample['anchor']])
            positive_emb_baseline = self.baseline_model.encode([sample['positive']])
            negative_emb_baseline = self.baseline_model.encode([sample['hard_negative']])
            
            pos_sim_baseline = cosine_similarity(anchor_emb_baseline, positive_emb_baseline)[0][0]
            neg_sim_baseline = cosine_similarity(anchor_emb_baseline, negative_emb_baseline)[0][0]
            
            trained_results.append({
                'pos_sim': pos_sim_trained,
                'neg_sim': neg_sim_trained,
                'correct': pos_sim_trained > neg_sim_trained
            })
            
            baseline_results.append({
                'pos_sim': pos_sim_baseline,
                'neg_sim': neg_sim_baseline,
                'correct': pos_sim_baseline > neg_sim_baseline
            })
        
        # å¯¹æ¯”ç»Ÿè®¡
        trained_accuracy = np.mean([r['correct'] for r in trained_results])
        baseline_accuracy = np.mean([r['correct'] for r in baseline_results])
        
        trained_gap = np.mean([r['pos_sim'] - r['neg_sim'] for r in trained_results])
        baseline_gap = np.mean([r['pos_sim'] - r['neg_sim'] for r in baseline_results])
        
        comparison_results = {
            'trained_model': {
                'accuracy': trained_accuracy,
                'similarity_gap': trained_gap,
                'avg_pos_sim': np.mean([r['pos_sim'] for r in trained_results]),
                'avg_neg_sim': np.mean([r['neg_sim'] for r in trained_results])
            },
            'baseline_model': {
                'accuracy': baseline_accuracy,
                'similarity_gap': baseline_gap,
                'avg_pos_sim': np.mean([r['pos_sim'] for r in baseline_results]),
                'avg_neg_sim': np.mean([r['neg_sim'] for r in baseline_results])
            },
            'improvement': {
                'accuracy_gain': trained_accuracy - baseline_accuracy,
                'similarity_gap_gain': trained_gap - baseline_gap,
                'relative_improvement': (trained_accuracy - baseline_accuracy) / baseline_accuracy * 100
            }
        }
        
        return comparison_results
    
    def visualize_results(self, results: Dict, comparison: Dict, output_dir: str):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. ç±»åˆ«æ€§èƒ½å¯¹æ¯”
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Aè‚¡RAGå¯¹æ¯”å­¦ä¹ æ¨¡å‹è¯„ä¼°ç»“æœ', fontsize=16)
        
        # ç±»åˆ«å‡†ç¡®ç‡
        categories = list(results['category_results'].keys())
        accuracies = [results['category_results'][cat]['accuracy'] for cat in categories]
        
        axes[0,0].bar(categories, accuracies, color='skyblue', alpha=0.7)
        axes[0,0].set_title('å„ç±»åˆ«å‡†ç¡®ç‡')
        axes[0,0].set_ylabel('å‡†ç¡®ç‡')
        axes[0,0].tick_params(axis='x', rotation=45)
        axes[0,0].grid(True, alpha=0.3)
        
        # ç›¸ä¼¼åº¦åˆ†å¸ƒ
        pos_sims = [results['category_results'][cat]['avg_pos_sim'] for cat in categories]
        neg_sims = [results['category_results'][cat]['avg_neg_sim'] for cat in categories]
        
        x = np.arange(len(categories))
        width = 0.35
        
        axes[0,1].bar(x - width/2, pos_sims, width, label='æ­£æ ·æœ¬ç›¸ä¼¼åº¦', color='green', alpha=0.7)
        axes[0,1].bar(x + width/2, neg_sims, width, label='è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦', color='red', alpha=0.7)
        axes[0,1].set_title('æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦å¯¹æ¯”')
        axes[0,1].set_ylabel('ä½™å¼¦ç›¸ä¼¼åº¦')
        axes[0,1].set_xticks(x)
        axes[0,1].set_xticklabels(categories, rotation=45)
        axes[0,1].legend()
        axes[0,1].grid(True, alpha=0.3)
        
        # æ¨¡å‹å¯¹æ¯”
        models = ['åŸºçº¿æ¨¡å‹', 'è®­ç»ƒæ¨¡å‹']
        baseline_acc = comparison['baseline_model']['accuracy']
        trained_acc = comparison['trained_model']['accuracy']
        
        axes[1,0].bar(models, [baseline_acc, trained_acc], 
                      color=['orange', 'green'], alpha=0.7)
        axes[1,0].set_title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”')
        axes[1,0].set_ylabel('å‡†ç¡®ç‡')
        axes[1,0].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate([baseline_acc, trained_acc]):
            axes[1,0].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # ç›¸ä¼¼åº¦å·®è·å¯¹æ¯”
        baseline_gap = comparison['baseline_model']['similarity_gap']
        trained_gap = comparison['trained_model']['similarity_gap']
        
        axes[1,1].bar(models, [baseline_gap, trained_gap],
                      color=['orange', 'green'], alpha=0.7)
        axes[1,1].set_title('ç›¸ä¼¼åº¦å·®è·å¯¹æ¯”')
        axes[1,1].set_ylabel('ç›¸ä¼¼åº¦å·®è·')
        axes[1,1].grid(True, alpha=0.3)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate([baseline_gap, trained_gap]):
            axes[1,1].text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(output_dir / 'evaluation_results.png', dpi=300, bbox_inches='tight')
        logger.info(f"è¯„ä¼°ç»“æœå›¾ä¿å­˜åˆ°: {output_dir / 'evaluation_results.png'}")
        plt.show()
        
        # 2. æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(8, 6))
        cm = np.array(results['confusion_matrix'])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                   xticklabels=['é¢„æµ‹é”™è¯¯', 'é¢„æµ‹æ­£ç¡®'],
                   yticklabels=['å®é™…é”™è¯¯', 'å®é™…æ­£ç¡®'])
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.ylabel('å®é™…æ ‡ç­¾')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.tight_layout()
        plt.savefig(output_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def generate_report(self, results: Dict, comparison: Dict, output_dir: str):
        """ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        report = {
            'evaluation_summary': {
                'model_performance': results,
                'baseline_comparison': comparison,
                'evaluation_date': pd.Timestamp.now().isoformat(),
                'total_test_samples': results['total_samples']
            }
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        report_path = output_dir / 'evaluation_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        md_content = f"""# Aè‚¡RAGå¯¹æ¯”å­¦ä¹ æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## è¯„ä¼°æ¦‚è§ˆ

- **è¯„ä¼°æ—¥æœŸ**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
- **æµ‹è¯•æ ·æœ¬æ•°**: {results['total_samples']}
- **è¯„ä¼°ä»»åŠ¡**: è¯­ä¹‰ç›¸ä¼¼åº¦åŒºåˆ†ä»»åŠ¡

## æ¨¡å‹æ€§èƒ½

### ä¸»è¦æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| å‡†ç¡®ç‡ | {results['accuracy']:.4f} |
| ç²¾ç¡®ç‡ | {results['precision']:.4f} |
| å¬å›ç‡ | {results['recall']:.4f} |
| F1åˆ†æ•° | {results['f1_score']:.4f} |
| AUC | {results['auc_score']:.4f} |
| æ­£æ ·æœ¬å¹³å‡ç›¸ä¼¼åº¦ | {results['avg_positive_similarity']:.4f} |
| è´Ÿæ ·æœ¬å¹³å‡ç›¸ä¼¼åº¦ | {results['avg_negative_similarity']:.4f} |
| ç›¸ä¼¼åº¦å·®è· | {results['similarity_gap']:.4f} |

### å„ç±»åˆ«æ€§èƒ½

| ç±»åˆ« | å‡†ç¡®ç‡ | ç›¸ä¼¼åº¦å·®è· | æ ·æœ¬æ•° |
|------|--------|------------|--------|
"""
        
        for category, cat_results in results['category_results'].items():
            md_content += f"| {category} | {cat_results['accuracy']:.4f} | {cat_results['similarity_gap']:.4f} | {cat_results['sample_count']} |\n"
        
        md_content += f"""
## åŸºçº¿æ¨¡å‹å¯¹æ¯”

### æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‡†ç¡®ç‡ | ç›¸ä¼¼åº¦å·®è· |
|------|--------|------------|
| åŸºçº¿æ¨¡å‹ | {comparison['baseline_model']['accuracy']:.4f} | {comparison['baseline_model']['similarity_gap']:.4f} |
| è®­ç»ƒæ¨¡å‹ | {comparison['trained_model']['accuracy']:.4f} | {comparison['trained_model']['similarity_gap']:.4f} |

### æ”¹è¿›æ•ˆæœ

- **å‡†ç¡®ç‡æå‡**: {comparison['improvement']['accuracy_gain']:.4f} ({comparison['improvement']['relative_improvement']:.2f}%)
- **ç›¸ä¼¼åº¦å·®è·æå‡**: {comparison['improvement']['similarity_gap_gain']:.4f}

## ç»“è®º

è®­ç»ƒåçš„å¯¹æ¯”å­¦ä¹ æ¨¡å‹åœ¨Aè‚¡èˆ†æƒ…è¯­ä¹‰åŒºåˆ†ä»»åŠ¡ä¸Šè¡¨ç°ä¼˜å¼‚ï¼Œç›¸æ¯”åŸºçº¿æ¨¡å‹æœ‰æ˜¾è‘—æå‡ï¼š

1. **å‡†ç¡®ç‡æå‡{comparison['improvement']['relative_improvement']:.1f}%**ï¼Œè¾¾åˆ°{results['accuracy']:.1%}
2. **ç›¸ä¼¼åº¦åŒºåˆ†èƒ½åŠ›å¢å¼º**ï¼Œæ­£è´Ÿæ ·æœ¬å·®è·ä»{comparison['baseline_model']['similarity_gap']:.3f}æå‡åˆ°{comparison['trained_model']['similarity_gap']:.3f}
3. **å„ç±»åˆ«è¡¨ç°å‡è¡¡**ï¼Œæ‰€æœ‰ç±»åˆ«å‡†ç¡®ç‡éƒ½æœ‰æ”¹å–„

è¯¥æ¨¡å‹å·²å‡†å¤‡å¥½ç”¨äºç”Ÿäº§ç¯å¢ƒçš„Aè‚¡èˆ†æƒ…RAGç³»ç»Ÿã€‚
"""
        
        md_path = output_dir / 'evaluation_report.md'
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        logger.info(f"è¯„ä¼°æŠ¥å‘Šä¿å­˜åˆ°: {report_path}")
        logger.info(f"MarkdownæŠ¥å‘Šä¿å­˜åˆ°: {md_path}")
        
        return report
    
    def run_full_evaluation(self, output_dir: str = "evaluation_results"):
        """è¿è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
        logger.info("ğŸ” å¼€å§‹å®Œæ•´æ€§èƒ½è¯„ä¼°...")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_samples = self.prepare_test_data()
        
        # ç›¸ä¼¼åº¦ä»»åŠ¡è¯„ä¼°
        similarity_results = self.evaluate_similarity_task(test_samples)
        
        # åŸºçº¿å¯¹æ¯”è¯„ä¼°
        comparison_results = self.evaluate_baseline_comparison(test_samples)
        
        # å¯è§†åŒ–ç»“æœ
        self.visualize_results(similarity_results, comparison_results, output_dir)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report(similarity_results, comparison_results, output_dir)
        
        # æ‰“å°ä¸»è¦ç»“æœ
        logger.info("ğŸ‰ è¯„ä¼°å®Œæˆï¼")
        logger.info(f"æ¨¡å‹å‡†ç¡®ç‡: {similarity_results['accuracy']:.4f}")
        logger.info(f"ç›¸ä¼¼åº¦å·®è·: {similarity_results['similarity_gap']:.4f}")
        logger.info(f"ç›¸æ¯”åŸºçº¿æå‡: {comparison_results['improvement']['relative_improvement']:.2f}%")
        
        return {
            'similarity_results': similarity_results,
            'comparison_results': comparison_results,
            'report': report
        }

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°RAGå¯¹æ¯”å­¦ä¹ æ¨¡å‹æ€§èƒ½")
    parser.add_argument("--model", type=str, help="è®­ç»ƒæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--config", type=str, help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, default="evaluation_results", 
                       help="ç»“æœè¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # è¿è¡Œè¯„ä¼°
    evaluator = RAGPerformanceEvaluator(
        model_path=args.model,
        config_path=args.config
    )
    
    results = evaluator.run_full_evaluation(args.output)

if __name__ == "__main__":
    main()
