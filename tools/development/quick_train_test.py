#!/usr/bin/env python3
"""
Aè‚¡RAGå¯¹æ¯”å­¦ä¹ å¿«é€Ÿè®­ç»ƒå’Œæµ‹è¯•è„šæœ¬
ä¸€é”®å®Œæˆç¯å¢ƒé…ç½®ã€æ¨¡å‹è®­ç»ƒã€æ€§èƒ½è¯„ä¼°çš„å®Œæ•´æµç¨‹
"""

import os
import sys
import subprocess
from pathlib import Path
import logging
import argparse
from datetime import datetime
import json
from typing import Tuple

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class QuickTrainer:
    """å¿«é€Ÿè®­ç»ƒç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.tools_dir = self.project_root / "tools"
        
        # è„šæœ¬è·¯å¾„
        self.setup_script = self.tools_dir / "development" / "setup" / "setup_training_environment.py"
        self.train_script = self.tools_dir / "development" / "training" / "train_contrastive_rag.py"
        self.eval_script = self.tools_dir / "development" / "testing" / "evaluate_rag_performance.py"
        
        # é…ç½®è·¯å¾„
        self.config_dir = self.project_root / "configs" / "training"
        self.config_file = self.config_dir / "contrastive_training.yaml"
        
        # è¾“å‡ºè·¯å¾„
        self.checkpoint_dir = self.project_root / "checkpoints"
        self.results_dir = self.project_root / "results"
        
    def check_prerequisites(self) -> bool:
        """æ£€æŸ¥å‰ç½®æ¡ä»¶"""
        logger.info("ğŸ” æ£€æŸ¥å‰ç½®æ¡ä»¶...")
        
        # æ£€æŸ¥è„šæœ¬æ–‡ä»¶
        required_scripts = [self.setup_script, self.train_script, self.eval_script]
        missing_scripts = []
        
        for script in required_scripts:
            if not script.exists():
                missing_scripts.append(str(script))
        
        if missing_scripts:
            logger.error(f"ç¼ºå°‘å¿…è¦è„šæœ¬æ–‡ä»¶: {missing_scripts}")
            return False
        
        # æ£€æŸ¥æ ·æœ¬æ•°æ®
        samples_file = self.project_root / "services" / "decision_engine" / "contrastive_learning_samples.py"
        if not samples_file.exists():
            logger.error(f"ç¼ºå°‘è®­ç»ƒæ ·æœ¬æ–‡ä»¶: {samples_file}")
            return False
        
        logger.info("âœ… å‰ç½®æ¡ä»¶æ£€æŸ¥é€šè¿‡")
        return True
    
    def setup_environment(self, force_download: bool = False) -> bool:
        """é…ç½®è®­ç»ƒç¯å¢ƒ"""
        logger.info("ğŸ”§ é…ç½®è®­ç»ƒç¯å¢ƒ...")
        
        try:
            cmd = [sys.executable, str(self.setup_script)]
            if force_download:
                cmd.append("--force-download")
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=self.project_root)
            
            if result.returncode == 0:
                logger.info("âœ… ç¯å¢ƒé…ç½®æˆåŠŸ")
                logger.info(result.stdout)
                return True
            else:
                logger.error("âŒ ç¯å¢ƒé…ç½®å¤±è´¥")
                logger.error(result.stderr)
                return False
                
        except Exception as e:
            logger.error(f"ç¯å¢ƒé…ç½®å¼‚å¸¸: {e}")
            return False
    
    def run_training(self, epochs: int = 10, batch_size: int = 32, 
                    learning_rate: float = 5e-5, resume: str = None) -> Tuple[bool, str]:
        """è¿è¡Œè®­ç»ƒ"""
        logger.info(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ (epochs={epochs}, batch_size={batch_size}, lr={learning_rate})...")
        
        try:
            # ç¡®ä¿é…ç½®æ–‡ä»¶å­˜åœ¨
            if not self.config_file.exists():
                logger.warning("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                self._create_default_config(epochs, batch_size, learning_rate)
            
            cmd = [sys.executable, str(self.train_script), "--config", str(self.config_file)]
            if resume:
                cmd.extend(["--resume", resume])
            
            # è¿è¡Œè®­ç»ƒ
            result = subprocess.run(cmd, cwd=self.project_root)
            
            if result.returncode == 0:
                # æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹
                best_model = self.checkpoint_dir / "best_model.pth"
                if best_model.exists():
                    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹: {best_model}")
                    return True, str(best_model)
                else:
                    logger.warning("è®­ç»ƒå®Œæˆä½†æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹")
                    return True, ""
            else:
                logger.error("âŒ è®­ç»ƒå¤±è´¥")
                return False, ""
                
        except Exception as e:
            logger.error(f"è®­ç»ƒå¼‚å¸¸: {e}")
            return False, ""
    
    def run_evaluation(self, model_path: str = None) -> bool:
        """è¿è¡Œè¯„ä¼°"""
        logger.info("ğŸ“Š å¼€å§‹è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        try:
            # åˆ›å»ºç»“æœç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            eval_output = self.results_dir / f"evaluation_{timestamp}"
            eval_output.mkdir(parents=True, exist_ok=True)
            
            cmd = [sys.executable, str(self.eval_script), "--output", str(eval_output)]
            
            if model_path and Path(model_path).exists():
                cmd.extend(["--model", model_path])
            
            if self.config_file.exists():
                cmd.extend(["--config", str(self.config_file)])
            
            result = subprocess.run(cmd, cwd=self.project_root)
            
            if result.returncode == 0:
                logger.info(f"âœ… è¯„ä¼°å®Œæˆï¼Œç»“æœä¿å­˜åœ¨: {eval_output}")
                
                # æ˜¾ç¤ºä¸»è¦ç»“æœ
                report_file = eval_output / "evaluation_report.json"
                if report_file.exists():
                    with open(report_file, 'r', encoding='utf-8') as f:
                        report = json.load(f)
                    
                    results = report['evaluation_summary']['model_performance']
                    comparison = report['evaluation_summary']['baseline_comparison']
                    
                    logger.info("ğŸ“Š ä¸»è¦è¯„ä¼°ç»“æœ:")
                    logger.info(f"   å‡†ç¡®ç‡: {results['accuracy']:.4f}")
                    logger.info(f"   F1åˆ†æ•°: {results['f1_score']:.4f}")
                    logger.info(f"   ç›¸ä¼¼åº¦å·®è·: {results['similarity_gap']:.4f}")
                    logger.info(f"   ç›¸æ¯”åŸºçº¿æå‡: {comparison['improvement']['relative_improvement']:.2f}%")
                
                return True
            else:
                logger.error("âŒ è¯„ä¼°å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"è¯„ä¼°å¼‚å¸¸: {e}")
            return False
    
    def _create_default_config(self, epochs: int, batch_size: int, learning_rate: float):
        """åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶"""
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        config_content = f"""# Aè‚¡RAGå¯¹æ¯”å­¦ä¹ è®­ç»ƒé…ç½®
model:
  base_model: "BAAI/bge-large-zh-v1.5"
  embedding_dim: 1024
  projection_dim: 1024

training:
  batch_size: {batch_size}
  learning_rate: {learning_rate}
  epochs: {epochs}
  warmup_steps: 100
  temperature: 0.05
  margin: 0.5
  hard_negative_weight: 2.0
  gradient_clip_norm: 1.0

data:
  train_split: 0.8
  val_split: 0.2
  max_seq_length: 512
  num_hard_negatives: 3

optimization:
  optimizer: "AdamW"
  weight_decay: 0.01
  scheduler: "cosine"
  min_lr: 1e-6

logging:
  log_steps: 50
  eval_steps: 200
  save_steps: 500
  tensorboard_dir: "{self.project_root / 'logs' / 'tensorboard'}"
  checkpoint_dir: "{self.checkpoint_dir}"

evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "auc"]
  top_k: [1, 3, 5, 10]
  similarity_threshold: 0.7
"""
        
        with open(self.config_file, 'w', encoding='utf-8') as f:
            f.write(config_content)
        
        logger.info(f"åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {self.config_file}")
    
    def run_full_pipeline(self, epochs: int = 10, batch_size: int = 32,
                         learning_rate: float = 5e-5, force_setup: bool = False,
                         resume: str = None) -> bool:
        """è¿è¡Œå®Œæ•´è®­ç»ƒ+è¯„ä¼°æµç¨‹"""
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´è®­ç»ƒå’Œè¯„ä¼°æµç¨‹...")
        
        start_time = datetime.now()
        
        try:
            # 1. æ£€æŸ¥å‰ç½®æ¡ä»¶
            if not self.check_prerequisites():
                return False
            
            # 2. é…ç½®ç¯å¢ƒ (å¦‚æœéœ€è¦)
            if force_setup or not self.config_file.exists():
                if not self.setup_environment():
                    return False
            
            # 3. è®­ç»ƒæ¨¡å‹
            success, model_path = self.run_training(epochs, batch_size, learning_rate, resume)
            if not success:
                return False
            
            # 4. è¯„ä¼°æ¨¡å‹
            if not self.run_evaluation(model_path):
                return False
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("ğŸ‰ å®Œæ•´æµç¨‹æ‰§è¡ŒæˆåŠŸï¼")
            logger.info(f"æ€»è€—æ—¶: {duration}")
            logger.info(f"æœ€ä½³æ¨¡å‹: {model_path}")
            logger.info(f"ç»“æœç›®å½•: {self.results_dir}")
            
            return True
            
        except Exception as e:
            logger.error(f"æµç¨‹æ‰§è¡Œå¼‚å¸¸: {e}")
            return False
    
    def show_usage_guide(self):
        """æ˜¾ç¤ºä½¿ç”¨æŒ‡å—"""
        guide = """
ğŸ¯ Aè‚¡RAGå¯¹æ¯”å­¦ä¹ è®­ç»ƒæŒ‡å—

ğŸ“‹ ä½¿ç”¨æ­¥éª¤:

1. å¿«é€Ÿå¼€å§‹ (æ¨èæ–°æ‰‹):
   python tools/development/quick_train_test.py --full-pipeline

2. ä»…é…ç½®ç¯å¢ƒ:
   python tools/development/quick_train_test.py --setup-only

3. ä»…è®­ç»ƒæ¨¡å‹:
   python tools/development/quick_train_test.py --train-only --epochs 20

4. ä»…è¯„ä¼°æ¨¡å‹:
   python tools/development/quick_train_test.py --eval-only --model checkpoints/best_model.pth

5. è‡ªå®šä¹‰è®­ç»ƒå‚æ•°:
   python tools/development/quick_train_test.py --full-pipeline --epochs 30 --batch-size 16 --learning-rate 1e-5

ğŸ“Š è®­ç»ƒå‚æ•°è¯´æ˜:
- epochs: è®­ç»ƒè½®æ•° (é»˜è®¤10, æ¨è10-30)
- batch-size: æ‰¹æ¬¡å¤§å° (é»˜è®¤32, æ ¹æ®GPUå†…å­˜è°ƒæ•´)
- learning-rate: å­¦ä¹ ç‡ (é»˜è®¤5e-5, èŒƒå›´1e-6åˆ°1e-4)

ğŸ’¡ è®­ç»ƒå»ºè®®:
- é¦–æ¬¡è®­ç»ƒæ¨èä½¿ç”¨é»˜è®¤å‚æ•°
- GPUå†…å­˜ä¸è¶³å¯å‡å°batch-size
- æ”¶æ•›æ…¢å¯å¢åŠ epochs
- è¿‡æ‹Ÿåˆå¯å‡å°learning-rate

ğŸ“ˆ è¾“å‡ºæ–‡ä»¶:
- checkpoints/: æ¨¡å‹æ£€æŸ¥ç‚¹
- results/: è¯„ä¼°ç»“æœå’ŒæŠ¥å‘Š
- logs/: è®­ç»ƒæ—¥å¿—å’Œå¯è§†åŒ–

ğŸ”§ æ•…éšœæ’æŸ¥:
- å¦‚æœç¯å¢ƒé…ç½®å¤±è´¥ï¼Œå¯æ‰‹åŠ¨å®‰è£…ä¾èµ–: pip install torch sentence-transformers scikit-learn
- å¦‚æœè®­ç»ƒä¸­æ–­ï¼Œå¯ä½¿ç”¨ --resume checkpoints/latest_checkpoint.pth ç»§ç»­è®­ç»ƒ
- å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå‡å°batch-sizeæˆ–ä½¿ç”¨CPUè®­ç»ƒ
"""
        print(guide)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="Aè‚¡RAGå¯¹æ¯”å­¦ä¹ å¿«é€Ÿè®­ç»ƒå’Œæµ‹è¯•")
    
    # æ“ä½œæ¨¡å¼
    parser.add_argument("--full-pipeline", action="store_true", help="è¿è¡Œå®Œæ•´è®­ç»ƒ+è¯„ä¼°æµç¨‹")
    parser.add_argument("--setup-only", action="store_true", help="ä»…é…ç½®ç¯å¢ƒ")
    parser.add_argument("--train-only", action="store_true", help="ä»…è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--eval-only", action="store_true", help="ä»…è¯„ä¼°æ¨¡å‹")
    parser.add_argument("--usage", action="store_true", help="æ˜¾ç¤ºä½¿ç”¨æŒ‡å—")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=10, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning-rate", type=float, default=5e-5, help="å­¦ä¹ ç‡")
    
    # å…¶ä»–é€‰é¡¹
    parser.add_argument("--model", type=str, help="è¯„ä¼°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--resume", type=str, help="æ¢å¤è®­ç»ƒæ£€æŸ¥ç‚¹")
    parser.add_argument("--force-setup", action="store_true", help="å¼ºåˆ¶é‡æ–°é…ç½®ç¯å¢ƒ")
    parser.add_argument("--project-root", type=str, help="é¡¹ç›®æ ¹ç›®å½•")
    
    args = parser.parse_args()
    
    if args.usage:
        trainer = QuickTrainer()
        trainer.show_usage_guide()
        return
    
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = QuickTrainer(args.project_root)
    
    # æ‰§è¡Œå¯¹åº”æ“ä½œ
    if args.full_pipeline:
        success = trainer.run_full_pipeline(
            epochs=args.epochs,
            batch_size=args.batch_size, 
            learning_rate=args.learning_rate,
            force_setup=args.force_setup,
            resume=args.resume
        )
        sys.exit(0 if success else 1)
        
    elif args.setup_only:
        success = trainer.setup_environment(args.force_setup)
        sys.exit(0 if success else 1)
        
    elif args.train_only:
        success, _ = trainer.run_training(
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            resume=args.resume
        )
        sys.exit(0 if success else 1)
        
    elif args.eval_only:
        success = trainer.run_evaluation(args.model)
        sys.exit(0 if success else 1)
        
    else:
        # é»˜è®¤æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()
        print("\nğŸ’¡ ä½¿ç”¨ --usage æŸ¥çœ‹è¯¦ç»†ä½¿ç”¨æŒ‡å—")

if __name__ == "__main__":
    main()
