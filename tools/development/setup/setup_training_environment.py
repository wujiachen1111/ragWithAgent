#!/usr/bin/env python3
"""
RAGå¯¹æ¯”å­¦ä¹ è®­ç»ƒç¯å¢ƒé…ç½®è„šæœ¬
è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€é…ç½®ç¯å¢ƒã€éªŒè¯ä¾èµ–
"""

import os
import sys
import subprocess
import torch
from pathlib import Path
from sentence_transformers import SentenceTransformer
import logging
from typing import List, Dict
import requests
from tqdm import tqdm

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class TrainingEnvironmentSetup:
    """è®­ç»ƒç¯å¢ƒé…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent.parent
        self.models_dir = self.project_root / "models"
        self.cache_dir = self.project_root / ".cache"
        
        # åˆ›å»ºå¿…è¦ç›®å½•
        self.models_dir.mkdir(exist_ok=True)
        self.cache_dir.mkdir(exist_ok=True)
        
        # æ¨¡å‹é…ç½®
        self.models_config = {
            "embedding_model": {
                "name": "BAAI/bge-large-zh-v1.5",
                "type": "sentence-transformers",
                "description": "ä¸­æ–‡å¤§è§„æ¨¡é¢„è®­ç»ƒæ–‡æœ¬åµŒå…¥æ¨¡å‹",
                "size": "1.3GB"
            },
            "reranker_model": {
                "name": "BAAI/bge-reranker-large",
                "type": "sentence-transformers", 
                "description": "ä¸­æ–‡æ–‡æœ¬é‡æ’åºæ¨¡å‹",
                "size": "1.1GB"
            }
        }
        
    def check_dependencies(self) -> Dict[str, bool]:
        """æ£€æŸ¥è®­ç»ƒä¾èµ–"""
        dependencies = {
            "torch": False,
            "sentence_transformers": False,
            "sklearn": False,
            "numpy": False,
            "tqdm": False,
            "matplotlib": False,
            "seaborn": False
        }
        
        for dep in dependencies:
            try:
                __import__(dep.replace("-", "_"))
                dependencies[dep] = True
                logger.info(f"âœ… {dep} å·²å®‰è£…")
            except ImportError:
                logger.warning(f"âŒ {dep} æœªå®‰è£…")
                
        return dependencies
    
    def install_dependencies(self):
        """å®‰è£…ç¼ºå¤±ä¾èµ–"""
        logger.info("ğŸ”§ å®‰è£…è®­ç»ƒä¾èµ–...")
        
        requirements = [
            "torch>=1.12.0",
            "sentence-transformers>=2.2.0", 
            "scikit-learn>=1.1.0",
            "numpy>=1.21.0",
            "tqdm>=4.64.0",
            "matplotlib>=3.5.0",
            "seaborn>=0.11.0",
            "tensorboard>=2.10.0",
            "wandb>=0.13.0"  # å¯é€‰ï¼šå®éªŒè¿½è¸ª
        ]
        
        for req in requirements:
            try:
                subprocess.run([sys.executable, "-m", "pip", "install", req], 
                             check=True, capture_output=True)
                logger.info(f"âœ… å®‰è£…æˆåŠŸ: {req}")
            except subprocess.CalledProcessError as e:
                logger.error(f"âŒ å®‰è£…å¤±è´¥: {req}, é”™è¯¯: {e}")
    
    def check_gpu_availability(self) -> Dict[str, any]:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        gpu_info = {
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "gpu_names": [],
            "memory_info": []
        }
        
        if gpu_info["cuda_available"]:
            for i in range(gpu_info["gpu_count"]):
                gpu_names = torch.cuda.get_device_name(i)
                gpu_info["gpu_names"].append(gpu_names)
                
                # GPUå†…å­˜ä¿¡æ¯
                memory_info = {
                    "total": torch.cuda.get_device_properties(i).total_memory,
                    "allocated": torch.cuda.memory_allocated(i),
                    "reserved": torch.cuda.memory_reserved(i)
                }
                gpu_info["memory_info"].append(memory_info)
                
                logger.info(f"ğŸš€ GPU {i}: {gpu_names}")
                logger.info(f"   å†…å­˜: {memory_info['total']/1024**3:.1f}GB æ€»é‡")
        else:
            logger.warning("âš ï¸  æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒï¼ˆè¾ƒæ…¢ï¼‰")
            
        return gpu_info
    
    def download_models(self, force_download: bool = False) -> Dict[str, bool]:
        """ä¸‹è½½å’Œç¼“å­˜è®­ç»ƒæ¨¡å‹"""
        download_results = {}
        
        for model_key, model_config in self.models_config.items():
            model_name = model_config["name"]
            model_path = self.models_dir / model_name.replace("/", "_")
            
            logger.info(f"ğŸ“¥ å‡†å¤‡ä¸‹è½½æ¨¡å‹: {model_name} ({model_config['size']})")
            
            try:
                if model_path.exists() and not force_download:
                    logger.info(f"âœ… æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {model_name}")
                    download_results[model_key] = True
                    continue
                
                # ä½¿ç”¨sentence-transformersè‡ªåŠ¨ä¸‹è½½å’Œç¼“å­˜
                logger.info(f"ğŸ”„ æ­£åœ¨ä¸‹è½½ {model_name}...")
                
                if model_config["type"] == "sentence-transformers":
                    model = SentenceTransformer(model_name, 
                                              cache_folder=str(self.cache_dir))
                    
                    # ä¿å­˜åˆ°æœ¬åœ°modelsç›®å½•
                    model.save(str(model_path))
                    logger.info(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {model_name}")
                    download_results[model_key] = True
                else:
                    logger.warning(f"âš ï¸  ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_config['type']}")
                    download_results[model_key] = False
                    
            except Exception as e:
                logger.error(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {model_name}, é”™è¯¯: {e}")
                download_results[model_key] = False
                
        return download_results
    
    def create_training_config(self) -> str:
        """åˆ›å»ºè®­ç»ƒé…ç½®æ–‡ä»¶"""
        config_path = self.project_root / "configs" / "training" / "contrastive_training.yaml"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        
        config_content = f"""# Aè‚¡RAGå¯¹æ¯”å­¦ä¹ è®­ç»ƒé…ç½®
model:
  base_model: "BAAI/bge-large-zh-v1.5"
  model_path: "{self.models_dir / 'BAAI_bge-large-zh-v1_5'}"
  embedding_dim: 1024
  projection_dim: 1024
  
training:
  batch_size: 32
  learning_rate: 5e-5
  epochs: 20
  warmup_steps: 100
  temperature: 0.05
  margin: 0.5
  hard_negative_weight: 2.0
  gradient_clip_norm: 1.0
  
data:
  train_split: 0.8
  val_split: 0.2
  max_seq_length: 512
  num_hard_negatives: 3
  
optimization:
  optimizer: "AdamW"
  weight_decay: 0.01
  scheduler: "cosine"
  min_lr: 1e-6
  
logging:
  log_steps: 50
  eval_steps: 200
  save_steps: 500
  tensorboard_dir: "{self.project_root / 'logs' / 'tensorboard'}"
  checkpoint_dir: "{self.project_root / 'checkpoints'}"
  
evaluation:
  metrics: ["accuracy", "precision", "recall", "f1", "auc"]
  top_k: [1, 3, 5, 10]
  similarity_threshold: 0.7
"""
        
        with open(config_path, 'w', encoding='utf-8') as f:
            f.write(config_content)
            
        logger.info(f"âœ… è®­ç»ƒé…ç½®æ–‡ä»¶åˆ›å»º: {config_path}")
        return str(config_path)
    
    def validate_environment(self) -> bool:
        """éªŒè¯è®­ç»ƒç¯å¢ƒå®Œæ•´æ€§"""
        logger.info("ğŸ” éªŒè¯è®­ç»ƒç¯å¢ƒ...")
        
        # æ£€æŸ¥ä¾èµ–
        deps = self.check_dependencies()
        missing_deps = [k for k, v in deps.items() if not v]
        
        if missing_deps:
            logger.error(f"âŒ ç¼ºå°‘ä¾èµ–: {missing_deps}")
            return False
        
        # æ£€æŸ¥GPU
        gpu_info = self.check_gpu_availability()
        
        # æ£€æŸ¥æ¨¡å‹
        model_path = self.models_dir / "BAAI_bge-large-zh-v1_5"
        if not model_path.exists():
            logger.error("âŒ åŸºç¡€æ¨¡å‹æœªä¸‹è½½")
            return False
            
        # æ£€æŸ¥æ ·æœ¬æ•°æ®
        samples_path = self.project_root / "services" / "decision_engine" / "contrastive_learning_samples.py"
        if not samples_path.exists():
            logger.error("âŒ è®­ç»ƒæ ·æœ¬æ–‡ä»¶ä¸å­˜åœ¨")
            return False
            
        logger.info("âœ… è®­ç»ƒç¯å¢ƒéªŒè¯é€šè¿‡")
        return True
    
    def setup_full_environment(self):
        """å®Œæ•´ç¯å¢ƒé…ç½®æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹é…ç½®RAGè®­ç»ƒç¯å¢ƒ...")
        
        # 1. æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
        logger.info("æ­¥éª¤ 1/5: æ£€æŸ¥ä¾èµ–...")
        deps = self.check_dependencies()
        missing_deps = [k for k, v in deps.items() if not v]
        
        if missing_deps:
            logger.info(f"éœ€è¦å®‰è£…ä¾èµ–: {missing_deps}")
            self.install_dependencies()
        
        # 2. æ£€æŸ¥GPU
        logger.info("æ­¥éª¤ 2/5: æ£€æŸ¥GPUç¯å¢ƒ...")
        self.check_gpu_availability()
        
        # 3. ä¸‹è½½æ¨¡å‹
        logger.info("æ­¥éª¤ 3/5: ä¸‹è½½è®­ç»ƒæ¨¡å‹...")
        download_results = self.download_models()
        
        failed_downloads = [k for k, v in download_results.items() if not v]
        if failed_downloads:
            logger.warning(f"âš ï¸  éƒ¨åˆ†æ¨¡å‹ä¸‹è½½å¤±è´¥: {failed_downloads}")
        
        # 4. åˆ›å»ºé…ç½®
        logger.info("æ­¥éª¤ 4/5: åˆ›å»ºè®­ç»ƒé…ç½®...")
        config_path = self.create_training_config()
        
        # 5. éªŒè¯ç¯å¢ƒ
        logger.info("æ­¥éª¤ 5/5: éªŒè¯ç¯å¢ƒå®Œæ•´æ€§...")
        is_valid = self.validate_environment()
        
        if is_valid:
            logger.info("ğŸ‰ è®­ç»ƒç¯å¢ƒé…ç½®å®Œæˆï¼")
            logger.info(f"ğŸ“„ é…ç½®æ–‡ä»¶: {config_path}")
            logger.info(f"ğŸ“ æ¨¡å‹ç›®å½•: {self.models_dir}")
            logger.info(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {self.cache_dir}")
            
            # è¾“å‡ºä¸‹ä¸€æ­¥æŒ‡å¯¼
            logger.info("\nğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
            logger.info("1. è¿è¡Œè®­ç»ƒè„šæœ¬: python tools/development/training/train_contrastive_rag.py")
            logger.info("2. ç›‘æ§è®­ç»ƒè¿‡ç¨‹: tensorboard --logdir logs/tensorboard")
            logger.info("3. è¯„ä¼°æ¨¡å‹æ•ˆæœ: python tools/development/testing/evaluate_rag_performance.py")
        else:
            logger.error("âŒ ç¯å¢ƒé…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            
        return is_valid

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="é…ç½®RAGè®­ç»ƒç¯å¢ƒ")
    parser.add_argument("--project-root", type=str, help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--force-download", action="store_true", help="å¼ºåˆ¶é‡æ–°ä¸‹è½½æ¨¡å‹")
    parser.add_argument("--check-only", action="store_true", help="ä»…æ£€æŸ¥ç¯å¢ƒï¼Œä¸è¿›è¡Œé…ç½®")
    
    args = parser.parse_args()
    
    setup = TrainingEnvironmentSetup(args.project_root)
    
    if args.check_only:
        setup.validate_environment()
    else:
        setup.setup_full_environment()

if __name__ == "__main__":
    main()
