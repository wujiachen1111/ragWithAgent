#!/usr/bin/env python3
"""
Aè‚¡RAGå¯¹æ¯”å­¦ä¹ æ¨¡å‹è®­ç»ƒè„šæœ¬
åŸºäºéš¾è´Ÿæ ·æœ¬çš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼Œæå‡RAGæ£€ç´¢ç²¾åº¦
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import numpy as np
import yaml
import json
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from datetime import datetime
import pickle
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score

# æ·»åŠ é¡¹ç›®è·¯å¾„ï¼ˆè¿è¡Œæ—¶ä½¿ç”¨ï¼‰ï¼Œå¹¶ä½¿ç”¨åŒ…è·¯å¾„ä¾›IDEé™æ€åˆ†æ
project_root = Path(__file__).parent.parent.parent.parent

from services.decision_engine.contrastive_learning_samples import A_STOCK_CONTRASTIVE_SAMPLES, SentimentLabel
from services.decision_engine.contrastive_rag_enhancer import ContrastiveEmbeddingModel, ContrastiveTrainingConfig

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡è®°å½•"""
    epoch: int
    step: int
    loss: float
    accuracy: float
    precision: float
    recall: float
    f1: float
    learning_rate: float
    timestamp: str

class ContrastiveDataset(Dataset):
    """å¯¹æ¯”å­¦ä¹ æ•°æ®é›†"""
    
    def __init__(self, samples: List[Dict], tokenizer=None, max_length: int = 512):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        return {
            'anchor': sample['anchor'],
            'positive': sample['positive'], 
            'hard_negative': sample['hard_negative'],
            'category': sample['category'],
            'anchor_label': sample['anchor_label'].value,
            'explanation': sample['explanation']
        }

class ContrastiveLoss(nn.Module):
    """å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°"""
    
    def __init__(self, temperature: float = 0.05, margin: float = 0.5):
        super().__init__()
        self.temperature = temperature
        self.margin = margin
        
    def forward(self, anchor_emb, positive_emb, negative_emb):
        """
        è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±
        Args:
            anchor_emb: é”šç‚¹åµŒå…¥ [batch_size, embedding_dim]
            positive_emb: æ­£æ ·æœ¬åµŒå…¥ [batch_size, embedding_dim]  
            negative_emb: è´Ÿæ ·æœ¬åµŒå…¥ [batch_size, embedding_dim]
        """
        batch_size = anchor_emb.size(0)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        pos_sim = torch.cosine_similarity(anchor_emb, positive_emb, dim=1)
        neg_sim = torch.cosine_similarity(anchor_emb, negative_emb, dim=1)
        
        # InfoNCEæŸå¤±
        pos_exp = torch.exp(pos_sim / self.temperature)
        neg_exp = torch.exp(neg_sim / self.temperature)
        
        # å¯¹æ¯”æŸå¤±ï¼šæ‹‰è¿‘æ­£æ ·æœ¬ï¼Œæ¨è¿œè´Ÿæ ·æœ¬
        loss_contrastive = -torch.log(pos_exp / (pos_exp + neg_exp + 1e-8))
        
        # è¾¹ç•ŒæŸå¤±ï¼šç¡®ä¿æ­£è´Ÿæ ·æœ¬å·®è·
        loss_margin = torch.clamp(self.margin - (pos_sim - neg_sim), min=0)
        
        # æ€»æŸå¤±
        total_loss = loss_contrastive.mean() + 0.5 * loss_margin.mean()
        
        return {
            'total_loss': total_loss,
            'contrastive_loss': loss_contrastive.mean(),
            'margin_loss': loss_margin.mean(),
            'pos_sim': pos_sim.mean(),
            'neg_sim': neg_sim.mean()
        }

class ContrastiveTrainer:
    """å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, config_path: str):
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
            
        # è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜è®¾å¤‡ (CUDA > MPS > CPU)
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        elif torch.backends.mps.is_available():
            self.device = torch.device('mps')
        else:
            self.device = torch.device('cpu')
            
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = ContrastiveEmbeddingModel(
            base_model_name=self.config['model']['base_model']
        ).to(self.device)
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.criterion = ContrastiveLoss(
            temperature=self.config['training']['temperature'],
            margin=self.config['training']['margin']
        )
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config['training']['learning_rate'],
            weight_decay=self.config['optimization']['weight_decay']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=self.config['training']['epochs'],
            eta_min=self.config['optimization']['min_lr']
        )
        
        # è®­ç»ƒè®°å½•
        self.training_history = []
        self.best_accuracy = 0.0
        self.best_model_path = None
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.checkpoint_dir = Path(self.config['logging']['checkpoint_dir'])
        self.log_dir = Path(self.config['logging']['tensorboard_dir'])
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
    def prepare_data(self) -> Tuple[DataLoader, DataLoader]:
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        
        # è½¬æ¢æ ·æœ¬æ•°æ®
        all_samples = []
        for category, samples in A_STOCK_CONTRASTIVE_SAMPLES.items():
            for sample in samples:
                all_samples.append({
                    'anchor': sample['anchor'],
                    'positive': sample.get('positive', sample['anchor']),  # å¦‚æœæ²¡æœ‰æ­£æ ·æœ¬ï¼Œç”¨é”šç‚¹æ›¿ä»£
                    'hard_negative': sample['hard_negative'],
                    'category': category,
                    'anchor_label': sample['anchor_label'],
                    'explanation': sample['explanation']
                })
        
        logger.info(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}")
        
        # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
        train_size = int(len(all_samples) * self.config['data']['train_split'])
        train_samples = all_samples[:train_size]
        val_samples = all_samples[train_size:]
        
        logger.info(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_samples)}")
        logger.info(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_samples)}")
        
        # åˆ›å»ºæ•°æ®é›†å’ŒDataLoader
        train_dataset = ContrastiveDataset(train_samples, max_length=self.config['data']['max_seq_length'])
        val_dataset = ContrastiveDataset(val_samples, max_length=self.config['data']['max_seq_length'])
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=True,
            num_workers=self.config['data'].get('num_workers', 4),
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config['training']['batch_size'],
            shuffle=False,
            num_workers=self.config['data'].get('num_workers', 4),
            pin_memory=True if self.device.type == 'cuda' else False
        )
        
        return train_loader, val_loader
    
    def train_epoch(self, train_loader: DataLoader, epoch: int) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        all_pos_sims = []
        all_neg_sims = []
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")
        
        for step, batch in enumerate(progress_bar):
            # æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
            anchor_texts = batch['anchor']
            positive_texts = batch['positive']
            negative_texts = batch['hard_negative']
            
            # è·å–åµŒå…¥
            anchor_emb = self.model.encode(anchor_texts)
            positive_emb = self.model.encode(positive_texts)
            negative_emb = self.model.encode(negative_texts)
            
            # è®¡ç®—æŸå¤±
            loss_dict = self.criterion(anchor_emb, positive_emb, negative_emb)
            loss = loss_dict['total_loss']
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config['training']['gradient_clip_norm'] > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config['training']['gradient_clip_norm']
                )
            
            self.optimizer.step()
            
            # è®°å½•æŒ‡æ ‡
            total_loss += loss.item()
            all_pos_sims.append(loss_dict['pos_sim'].item())
            all_neg_sims.append(loss_dict['neg_sim'].item())
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Pos_Sim': f"{loss_dict['pos_sim'].item():.3f}",
                'Neg_Sim': f"{loss_dict['neg_sim'].item():.3f}"
            })
            
        epoch_metrics = {
            'avg_loss': total_loss / len(train_loader),
            'avg_pos_sim': np.mean(all_pos_sims),
            'avg_neg_sim': np.mean(all_neg_sims),
            'learning_rate': self.optimizer.param_groups[0]['lr']
        }
        
        return epoch_metrics
    
    def evaluate(self, val_loader: DataLoader) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_labels = []
        all_pos_sims = []
        all_neg_sims = []
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Evaluating"):
                anchor_texts = batch['anchor']
                positive_texts = batch['positive']
                negative_texts = batch['hard_negative']
                
                # è·å–åµŒå…¥
                anchor_emb = self.model.encode(anchor_texts)
                positive_emb = self.model.encode(positive_texts)
                negative_emb = self.model.encode(negative_texts)
                
                # è®¡ç®—æŸå¤±
                loss_dict = self.criterion(anchor_emb, positive_emb, negative_emb)
                total_loss += loss_dict['total_loss'].item()
                
                # è®¡ç®—é¢„æµ‹å‡†ç¡®æ€§ï¼ˆæ­£æ ·æœ¬ç›¸ä¼¼åº¦åº”è¯¥é«˜äºè´Ÿæ ·æœ¬ï¼‰
                pos_sims = torch.cosine_similarity(anchor_emb, positive_emb, dim=1)
                neg_sims = torch.cosine_similarity(anchor_emb, negative_emb, dim=1)
                
                predictions = (pos_sims > neg_sims).float()  # 1 è¡¨ç¤ºé¢„æµ‹æ­£ç¡®
                all_predictions.extend(predictions.cpu().tolist())
                all_labels.extend([1] * len(predictions))  # å…¨éƒ¨æ ‡ç­¾ä¸º1ï¼ˆæ­£ç¡®ï¼‰
                
                all_pos_sims.extend(pos_sims.cpu().tolist())
                all_neg_sims.extend(neg_sims.cpu().tolist())
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        accuracy = np.mean(all_predictions)
        avg_pos_sim = np.mean(all_pos_sims)
        avg_neg_sim = np.mean(all_neg_sims)
        similarity_gap = avg_pos_sim - avg_neg_sim
        
        eval_metrics = {
            'avg_loss': total_loss / len(val_loader),
            'accuracy': accuracy,
            'avg_pos_sim': avg_pos_sim,
            'avg_neg_sim': avg_neg_sim,
            'similarity_gap': similarity_gap,
            'num_samples': len(all_predictions)
        }
        
        return eval_metrics
    
    def save_checkpoint(self, epoch: int, metrics: Dict, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'metrics': metrics,
            'config': self.config,
            'training_history': self.training_history
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        latest_path = self.checkpoint_dir / 'latest_checkpoint.pth'
        torch.save(checkpoint, latest_path)
        logger.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: {latest_path}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_path = self.checkpoint_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
            self.best_model_path = best_path
            logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {best_path}")
            
        # å®šæœŸä¿å­˜
        if (epoch + 1) % 5 == 0:
            epoch_path = self.checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth'
            torch.save(checkpoint, epoch_path)
    
    def load_checkpoint(self, checkpoint_path: str) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_path):
            logger.warning(f"æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return 0
            
        checkpoint = torch.load(checkpoint_path, map_location=self.device)  # TODO: Consider using weights_only=True for security
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.training_history = checkpoint.get('training_history', [])
        
        epoch = checkpoint['epoch']
        logger.info(f"åŠ è½½æ£€æŸ¥ç‚¹æˆåŠŸï¼Œä»epoch {epoch+1}ç»§ç»­è®­ç»ƒ")
        return epoch + 1
    
    def plot_training_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        if not self.training_history:
            return
            
        # æå–æ•°æ®
        epochs = [m['epoch'] for m in self.training_history]
        train_losses = [m['train_loss'] for m in self.training_history]
        val_losses = [m['val_loss'] for m in self.training_history]
        accuracies = [m['val_accuracy'] for m in self.training_history]
        similarity_gaps = [m['similarity_gap'] for m in self.training_history]
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('è®­ç»ƒå†å²', fontsize=16)
        
        # æŸå¤±æ›²çº¿
        axes[0,0].plot(epochs, train_losses, label='è®­ç»ƒæŸå¤±', color='blue')
        axes[0,0].plot(epochs, val_losses, label='éªŒè¯æŸå¤±', color='red')
        axes[0,0].set_title('æŸå¤±æ›²çº¿')
        axes[0,0].set_xlabel('Epoch')
        axes[0,0].set_ylabel('Loss')
        axes[0,0].legend()
        axes[0,0].grid(True)
        
        # å‡†ç¡®ç‡æ›²çº¿
        axes[0,1].plot(epochs, accuracies, label='éªŒè¯å‡†ç¡®ç‡', color='green')
        axes[0,1].set_title('å‡†ç¡®ç‡æ›²çº¿')
        axes[0,1].set_xlabel('Epoch')
        axes[0,1].set_ylabel('Accuracy')
        axes[0,1].legend()
        axes[0,1].grid(True)
        
        # ç›¸ä¼¼åº¦å·®è·
        axes[1,0].plot(epochs, similarity_gaps, label='ç›¸ä¼¼åº¦å·®è·', color='purple')
        axes[1,0].set_title('æ­£è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦å·®è·')
        axes[1,0].set_xlabel('Epoch')
        axes[1,0].set_ylabel('Similarity Gap')
        axes[1,0].legend()
        axes[1,0].grid(True)
        
        # å­¦ä¹ ç‡æ›²çº¿
        learning_rates = [m['learning_rate'] for m in self.training_history]
        axes[1,1].plot(epochs, learning_rates, label='å­¦ä¹ ç‡', color='orange')
        axes[1,1].set_title('å­¦ä¹ ç‡å˜åŒ–')
        axes[1,1].set_xlabel('Epoch')
        axes[1,1].set_ylabel('Learning Rate')
        axes[1,1].legend()
        axes[1,1].grid(True)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        plot_path = self.log_dir / 'training_history.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        logger.info(f"è®­ç»ƒå†å²å›¾è¡¨ä¿å­˜åˆ°: {plot_path}")
        plt.show()
    
    def train(self, resume_from: str = None):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸš€ å¼€å§‹Aè‚¡RAGå¯¹æ¯”å­¦ä¹ è®­ç»ƒ...")
        
        # å‡†å¤‡æ•°æ®
        train_loader, val_loader = self.prepare_data()
        
        # æ¢å¤è®­ç»ƒ
        start_epoch = 0
        if resume_from:
            start_epoch = self.load_checkpoint(resume_from)
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(start_epoch, self.config['training']['epochs']):
            logger.info(f"\n========== Epoch {epoch+1}/{self.config['training']['epochs']} ==========")
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(train_loader, epoch)
            
            # éªŒè¯
            val_metrics = self.evaluate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # è®°å½•æŒ‡æ ‡
            epoch_record = {
                'epoch': epoch + 1,
                'train_loss': train_metrics['avg_loss'],
                'val_loss': val_metrics['avg_loss'],
                'val_accuracy': val_metrics['accuracy'],
                'similarity_gap': val_metrics['similarity_gap'],
                'learning_rate': train_metrics['learning_rate'],
                'timestamp': datetime.now().isoformat()
            }
            self.training_history.append(epoch_record)
            
            # æ‰“å°æŒ‡æ ‡
            logger.info(f"è®­ç»ƒæŸå¤±: {train_metrics['avg_loss']:.4f}")
            logger.info(f"éªŒè¯æŸå¤±: {val_metrics['avg_loss']:.4f}")  
            logger.info(f"éªŒè¯å‡†ç¡®ç‡: {val_metrics['accuracy']:.4f}")
            logger.info(f"ç›¸ä¼¼åº¦å·®è·: {val_metrics['similarity_gap']:.4f}")
            logger.info(f"å­¦ä¹ ç‡: {train_metrics['learning_rate']:.2e}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_metrics['accuracy'] > self.best_accuracy
            if is_best:
                self.best_accuracy = val_metrics['accuracy']
                logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {self.best_accuracy:.4f}")
            
            self.save_checkpoint(epoch, val_metrics, is_best)
            
        logger.info("âœ… è®­ç»ƒå®Œæˆ!")
        logger.info(f"æœ€ä½³å‡†ç¡®ç‡: {self.best_accuracy:.4f}")
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        self.plot_training_history()
        
        # ä¿å­˜è®­ç»ƒè®°å½•
        history_path = self.log_dir/ 'training_history.json'
        with open(history_path, 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, ensure_ascii=False, indent=2)
        logger.info(f"è®­ç»ƒè®°å½•ä¿å­˜åˆ°: {history_path}")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="è®­ç»ƒAè‚¡RAGå¯¹æ¯”å­¦ä¹ æ¨¡å‹")
    parser.add_argument("--config", type=str, 
                       default="configs/training/contrastive_training.yaml",
                       help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--resume", type=str, help="æ¢å¤è®­ç»ƒçš„æ£€æŸ¥ç‚¹è·¯å¾„")
    parser.add_argument("--gpu", type=int, help="ä½¿ç”¨çš„GPU ID")
    
    args = parser.parse_args()
    
    # è®¾ç½®GPU
    if args.gpu is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(args.gpu)
    
    # å¼€å§‹è®­ç»ƒ
    trainer = ContrastiveTrainer(args.config)
    trainer.train(resume_from=args.resume)

if __name__ == "__main__":
    main()
