import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone, timedelta
from yuqing.core.logging import app_logger
from yuqing.core.database import get_db
from yuqing.models.database_models import NewsItem
from yuqing.services.google_news_service import google_news_service
from yuqing.services.hot_news_discovery import hot_news_discovery
from yuqing.services.cailian_news_service import cailian_news_service
from yuqing.services.entity_analysis_orchestrator import entity_analysis_orchestrator
from sqlalchemy.orm import Session


class DataCollectionOrchestrator:
    """æ•°æ®é‡‡é›†åè°ƒå™¨ - ç»Ÿä¸€ç®¡ç†Googleæ–°é—»å’Œçƒ­ç‚¹å‘ç°"""
    
    def __init__(self):
        self.services = {
            "google_news": google_news_service,
            "hot_discovery": hot_news_discovery,
            "cailian_news": cailian_news_service
        }
        
        # é‡‡é›†é…ç½® - ç»Ÿä¸€5åˆ†é’Ÿè°ƒåº¦
        self.collection_config = {
            "hot_discovery": {
                "enabled": True,
                "max_items": 50,
                "hours_back": 12,  # æ‰©å±•åˆ°12å°æ—¶
                "priority": 1,  # æœ€é«˜ä¼˜å…ˆçº§
                "description": "æ™ºèƒ½çƒ­ç‚¹å‘ç°"
            },
            "cailian_news": {
                "enabled": True,
                "max_items": 150,
                "priority": 2,  # ç¬¬äºŒä¼˜å…ˆçº§
                "description": "è´¢è”ç¤¾ä¸“ä¸šè´¢ç»æ–°é—»"
            },
            "google_news": {
                "enabled": True,
                "max_items": 50,
                "keywords": ["è‚¡ç¥¨", "è´¢ç»", "æŠ•èµ„", "Aè‚¡", "æ¸¯è‚¡", "ç¾è‚¡"],
                "sort_by_date": True,  # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°
                "priority": 3,  # ç¬¬ä¸‰ä¼˜å…ˆçº§
                "description": "Googleæ–°é—»æœç´¢"
            }
        }
    
    async def collect_all_sources(self) -> Dict[str, Any]:
        """é‡‡é›†æ‰€æœ‰æ•°æ®æº"""
        results = {
            "timestamp": datetime.now(timezone.utc),
            "sources": {},
            "total_items": 0,
            "errors": []
        }
        
        app_logger.info("å¼€å§‹å…¨é‡æ•°æ®é‡‡é›†...")
        app_logger.info("ğŸ“‹ é‡‡é›†é…ç½®:")
        for source_name, config in self.collection_config.items():
            if config.get("enabled", True):
                max_items = config.get("max_items", "æœªé™åˆ¶")
                keywords = config.get("keywords", [])
                keyword_info = f", å…³é”®è¯: {len(keywords)}ä¸ª" if keywords else ""
                app_logger.info(f"  {source_name}: æœ€å¤§{max_items}æ¡{keyword_info}")
        
        # å¹¶è¡Œé‡‡é›†æ‰€æœ‰æ•°æ®æº
        tasks = []
        for source_name, config in self.collection_config.items():
            if config.get("enabled", True):
                task = asyncio.create_task(
                    self._collect_single_source(source_name, config),
                    name=f"collect_{source_name}"
                )
                tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for i, result in enumerate(completed_tasks):
            source_name = list(self.collection_config.keys())[i]
            
            if isinstance(result, Exception):
                error_msg = f"{source_name} é‡‡é›†å¤±è´¥: {result}"
                app_logger.error(error_msg)
                results["errors"].append(error_msg)
                results["sources"][source_name] = {"items": 0, "error": str(result)}
            else:
                results["sources"][source_name] = result
                results["total_items"] += result.get("items", 0)
        
        app_logger.info("ğŸ“Š æ•°æ®é‡‡é›†ç»“æœç»Ÿè®¡:")
        app_logger.info(f"  æ€»è®¡é‡‡é›†: {results['total_items']} æ¡æ–°é—»")
        
        for source_name, source_result in results["sources"].items():
            if isinstance(source_result, dict):
                items = source_result.get("items", 0)
                saved = source_result.get("saved_to_db", 0)
                duration = source_result.get("duration_seconds", 0)
                success = "âœ…" if source_result.get("success") else "âŒ"
                
                app_logger.info(f"  {success} {source_name}: {items}æ¡é‡‡é›† -> {saved}æ¡ä¿å­˜ ({duration:.1f}s)")
                
                if not source_result.get("success") and "error" in source_result:
                    app_logger.warning(f"    é”™è¯¯: {source_result['error']}")
        
        if results.get("errors"):
            app_logger.warning(f"  âš ï¸ é‡‡é›†é”™è¯¯: {'; '.join(results['errors'])}")
        
        return results
    
    async def _collect_single_source(self, source_name: str, 
                                   config: Dict[str, Any]) -> Dict[str, Any]:
        """é‡‡é›†å•ä¸ªæ•°æ®æº"""
        start_time = datetime.now()
        
        try:
            if source_name == "hot_discovery":
                # æ™ºèƒ½çƒ­ç‚¹å‘ç°
                hours_back = config.get("hours_back", 6)
                
                items = await hot_news_discovery.discover_hot_news(hours_back)
                
                # å»é‡å’Œä¿å­˜åˆ°æ•°æ®åº“
                if items:
                    unique_items = hot_news_discovery._enhanced_deduplicate(items)
                    saved_count = await hot_news_discovery.save_to_database(unique_items)
                else:
                    saved_count = 0
                
                return {
                    "items": len(items),
                    "saved_to_db": saved_count,
                    "duration_seconds": (datetime.now() - start_time).total_seconds(),
                    "success": True
                }
                
            elif source_name == "cailian_news":
                # è´¢è”ç¤¾æ–°é—»é‡‡é›†
                max_items = config.get("max_items", 100)
                
                items = await cailian_news_service.fetch_cailian_news(max_items)
                # ä¿å­˜åˆ°æ•°æ®åº“
                if items:
                    saved_count = await cailian_news_service.save_to_database(items)
                else:
                    saved_count = 0
                
                return {
                    "items": len(items),
                    "saved_to_db": saved_count,
                    "duration_seconds": (datetime.now() - start_time).total_seconds(),
                    "success": True
                }
                
            elif source_name == "google_news":
                # Googleæ–°é—»æœç´¢ - ä¼˜åŒ–æ•°é‡æ§åˆ¶
                keywords = config.get("keywords", ["è‚¡ç¥¨", "è´¢ç»"])
                max_items = config.get("max_items", 50)
                
                app_logger.info(f"Googleæ–°é—»é‡‡é›†: {len(keywords)}ä¸ªå…³é”®è¯, æ€»é™åˆ¶{max_items}æ¡")
                
                all_items = []
                items_per_keyword = max(5, max_items // len(keywords))  # æ¯ä¸ªå…³é”®è¯è‡³å°‘5æ¡
                
                for i, keyword in enumerate(keywords):
                    try:
                        # ä¸ºäº†è·å–æœ€æ–°æ–°é—»ï¼Œé™åˆ¶æ¯ä¸ªå…³é”®è¯çš„æ•°é‡
                        items = await google_news_service.search_news(keyword, items_per_keyword)
                        app_logger.info(f"  å…³é”®è¯ '{keyword}': é‡‡é›† {len(items)} æ¡")
                        all_items.extend(items)
                        
                        # é¿å…è¿‡å¤šé‡‡é›†ï¼Œè¾¾åˆ°é™åˆ¶å°±åœæ­¢
                        if len(all_items) >= max_items:
                            app_logger.info(f"  å·²è¾¾åˆ°æœ€å¤§é™åˆ¶ {max_items} æ¡ï¼Œåœæ­¢é‡‡é›†")
                            break
                            
                    except Exception as e:
                        app_logger.error(f"Googleæ–°é—»å…³é”®è¯ {keyword} é‡‡é›†å¤±è´¥: {e}")
                
                # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
                if all_items:
                    # æŒ‰å‘å¸ƒæ—¶é—´é™åºæ’åº (æœ€æ–°çš„åœ¨å‰)
                    all_items.sort(key=lambda x: x.get('published_at', datetime.min.replace(tzinfo=timezone.utc)), reverse=True)
                    
                    # é™åˆ¶æ€»æ•°é‡
                    all_items = all_items[:max_items]
                    
                    app_logger.info(f"Googleæ–°é—»æ’åºåä¿ç•™æœ€æ–° {len(all_items)} æ¡")
                    
                    # å»é‡å’Œä¿å­˜åˆ°æ•°æ®åº“
                    unique_items = google_news_service._enhanced_deduplicate(all_items)
                    saved_count = await google_news_service.save_to_database(unique_items)
                else:
                    saved_count = 0
                
                return {
                    "items": len(all_items),
                    "saved_to_db": saved_count,
                    "duration_seconds": (datetime.now() - start_time).total_seconds(),
                    "success": True
                }
            
            else:
                app_logger.warning(f"æœªçŸ¥æ•°æ®æº: {source_name}")
                return {
                    "items": 0,
                    "duration_seconds": 0,
                    "success": False,
                    "error": f"æœªçŸ¥æ•°æ®æº: {source_name}"
                }
                
        except Exception as e:
            app_logger.error(f"{source_name} é‡‡é›†å¼‚å¸¸: {e}")
            return {
                "items": 0,
                "duration_seconds": (datetime.now() - start_time).total_seconds(),
                "success": False,
                "error": str(e)
            }
    
    async def collect_cailian_news(self, limit: int = 100) -> List[Dict[str, Any]]:
        """è´¢è”ç¤¾æ–°é—»é‡‡é›†"""
        return await cailian_news_service.fetch_cailian_news(limit)
    
    async def collect_google_news(self, keywords: Optional[List[str]] = None, 
                                max_items: int = 50) -> List[Dict[str, Any]]:
        """Googleæ–°é—»é‡‡é›†"""
        if not keywords:
            keywords = ["è‚¡ç¥¨", "è´¢ç»", "æŠ•èµ„"]
        
        all_items = []
        for keyword in keywords:
            try:
                items = await google_news_service.search_news(keyword, max_items // len(keywords))
                all_items.extend(items)
            except Exception as e:
                app_logger.error(f"Googleæ–°é—»å…³é”®è¯ {keyword} é‡‡é›†å¤±è´¥: {e}")
        
        return all_items
    
    async def collect_hot_discovery(self, hours_back: int = 6, 
                                  max_items: int = 30) -> List[Dict[str, Any]]:
        """æ™ºèƒ½çƒ­ç‚¹å‘ç°"""
        return await hot_news_discovery.discover_hot_news(hours_back=hours_back, max_items=max_items)
    
    async def collect_by_keywords(self, keywords: List[str], 
                                max_items: int = 50) -> Dict[str, Any]:
        """æŒ‰å…³é”®è¯é‡‡é›†æ•°æ®"""
        results = {
            "timestamp": datetime.now(timezone.utc),
            "keywords": keywords,
            "sources": {},
            "total_items": 0,
            "errors": []
        }
        
        app_logger.info(f"å¼€å§‹å…³é”®è¯é‡‡é›†: {keywords}")
        
        # è´¢è”ç¤¾æ–°é—»é‡‡é›†
        try:
            cailian_items = await self.collect_cailian_news(max_items)
            results["sources"]["cailian_news"] = {
                "items": len(cailian_items),
                "success": True
            }
            results["total_items"] += len(cailian_items)
        except Exception as e:
            error_msg = f"è´¢è”ç¤¾æ–°é—»é‡‡é›†å¤±è´¥: {e}"
            app_logger.error(error_msg)
            results["errors"].append(error_msg)
            results["sources"]["cailian_news"] = {"items": 0, "error": str(e)}
        
        # Googleæ–°é—»é‡‡é›†
        try:
            google_items = await self.collect_google_news(keywords, max_items)
            results["sources"]["google_news"] = {
                "items": len(google_items),
                "success": True
            }
            results["total_items"] += len(google_items)
        except Exception as e:
            error_msg = f"Googleæ–°é—»é‡‡é›†å¤±è´¥: {e}"
            app_logger.error(error_msg)
            results["errors"].append(error_msg)
            results["sources"]["google_news"] = {"items": 0, "error": str(e)}
        
        app_logger.info(f"å…³é”®è¯é‡‡é›†å®Œæˆï¼Œæ€»è®¡ {results['total_items']} æ¡æ•°æ®")
        return results
    
    async def collect_by_companies(self, companies: List[str], 
                                 max_items: int = 50) -> Dict[str, Any]:
        """æŒ‰å…¬å¸åç§°é‡‡é›†æ•°æ®"""
        results = {
            "timestamp": datetime.now(timezone.utc),
            "companies": companies,
            "sources": {},
            "total_items": 0,
            "errors": []
        }
        
        app_logger.info(f"å¼€å§‹å…¬å¸æ–°é—»é‡‡é›†: {companies}")
        
        # è´¢è”ç¤¾æ–°é—»é‡‡é›†
        try:
            cailian_items = await self.collect_cailian_news(max_items)
            results["sources"]["cailian_news"] = {
                "items": len(cailian_items),
                "success": True
            }
            results["total_items"] += len(cailian_items)
        except Exception as e:
            error_msg = f"è´¢è”ç¤¾æ–°é—»é‡‡é›†å¤±è´¥: {e}"
            app_logger.error(error_msg)
            results["errors"].append(error_msg)
            results["sources"]["cailian_news"] = {"items": 0, "error": str(e)}
        
        # Googleæ–°é—»é‡‡é›†
        try:
            google_items = await self.collect_google_news(companies, max_items)
            results["sources"]["google_news"] = {
                "items": len(google_items),
                "success": True
            }
            results["total_items"] += len(google_items)
        except Exception as e:
            error_msg = f"Googleæ–°é—»é‡‡é›†å¤±è´¥: {e}"
            app_logger.error(error_msg)
            results["errors"].append(error_msg)
            results["sources"]["google_news"] = {"items": 0, "error": str(e)}
        
        app_logger.info(f"å…¬å¸æ–°é—»é‡‡é›†å®Œæˆï¼Œæ€»è®¡ {results['total_items']} æ¡æ•°æ®")
        return results
    
    async def get_collection_status(self) -> Dict[str, Any]:
        """è·å–é‡‡é›†çŠ¶æ€"""
        status = {
            "timestamp": datetime.now(timezone.utc),
            "services": {},
            "database_status": {}
        }
        
        # æ£€æŸ¥æœåŠ¡çŠ¶æ€
        for service_name, service in self.services.items():
            try:
                # ç®€å•çš„å¥åº·æ£€æŸ¥
                status["services"][service_name] = {
                    "status": "active",
                    "enabled": self.collection_config.get(service_name, {}).get("enabled", True)
                }
            except Exception as e:
                status["services"][service_name] = {
                    "status": "error",
                    "error": str(e),
                    "enabled": False
                }
        
        # æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
        try:
            db = next(get_db())
            recent_count = db.query(NewsItem).filter(
                NewsItem.collected_at != None  # noqa: E711
            ).filter(
                NewsItem.collected_at >= datetime.now(timezone.utc) - timedelta(hours=24)
            ).count()
            
            total_count = db.query(NewsItem).count()
            
            status["database_status"] = {
                "status": "connected",
                "total_news": total_count,
                "recent_24h": recent_count
            }
            
        except Exception as e:
            status["database_status"] = {
                "status": "error",
                "error": str(e)
            }
        
        return status
    
    async def analyze_collected_news(self, hours_back: int = 1, limit: int = 20, source: Optional[str] = None) -> Dict[str, Any]:
        """åˆ†ææœ€è¿‘é‡‡é›†çš„æ–°é—»ï¼ˆåŒ…å«å®ä½“è¯†åˆ«ï¼‰ã€‚

        å…¼å®¹è€æ•°æ®ï¼šå½“ collected_at ä¸ºç©ºæ—¶ï¼Œä»¥ published_at ä½œä¸ºå›é€€æ—¶é—´å­—æ®µã€‚
        """
        try:
            db = next(get_db())
            
            # è·å–æœ€è¿‘é‡‡é›†çš„æ–°é—»
            cutoff_time = datetime.now(timezone.utc) - timedelta(hours=hours_back)
            q = db.query(NewsItem).filter(
                (NewsItem.collected_at != None) & (NewsItem.collected_at >= cutoff_time)  # noqa: E711
                |
                ((NewsItem.collected_at == None) & (NewsItem.published_at != None) & (NewsItem.published_at >= cutoff_time))  # noqa: E711
            )
            if source:
                q = q.filter(NewsItem.source == source)
            recent_news = (
                q.order_by(NewsItem.published_at.desc().nullslast())
                 .limit(limit)
                 .all()
            )  # é™åˆ¶åˆ†ææ•°é‡ï¼Œé¿å…APIè°ƒç”¨è¿‡å¤š
            
            if not recent_news:
                return {
                    "success": True,
                    "message": "æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ—¶é—´çª—å£çš„æ–°é—»ï¼ˆè¯·å…ˆé‡‡é›†æˆ–æ‰©å¤§ hours_backï¼‰",
                    "analyzed_count": 0
                }
            
            app_logger.info(f"å¼€å§‹åˆ†ææœ€è¿‘ {len(recent_news)} æ¡æ–°é—»")
            
            # ä½¿ç”¨å®ä½“åˆ†æç¼–æ’å™¨è¿›è¡Œç»¼åˆåˆ†æ
            analysis_result = await entity_analysis_orchestrator.batch_analyze_news(
                recent_news, batch_size=5
            )
            
            return {
                "success": True,
                "analyzed_count": analysis_result["successful_count"],
                "failed_count": analysis_result["failed_count"],
                "analysis_details": analysis_result
            }
            
        except Exception as e:
            app_logger.error(f"æ–°é—»åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def full_pipeline_execution(self, enable_analysis: bool = True) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“ï¼šé‡‡é›† + åˆ†æ"""
        pipeline_start = datetime.now()
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®é‡‡é›†
            app_logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“ - é˜¶æ®µ1: æ•°æ®é‡‡é›†")
            collection_result = await self.collect_all_sources()
            
            if not collection_result["success"]:
                return {
                    "success": False,
                    "stage": "collection",
                    "error": "æ•°æ®é‡‡é›†å¤±è´¥",
                    "details": collection_result
                }
            
            # ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®åˆ†æï¼ˆå¯é€‰ï¼‰
            analysis_result = None
            if enable_analysis and collection_result["total_items"] > 0:
                app_logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“ - é˜¶æ®µ2: å®ä½“åˆ†æ")
                analysis_result = await self.analyze_collected_news(hours_back=1)
            
            pipeline_duration = (datetime.now() - pipeline_start).total_seconds()
            
            return {
                "success": True,
                "pipeline_duration": pipeline_duration,
                "collection_result": collection_result,
                "analysis_result": analysis_result,
                "summary": {
                    "collected_items": collection_result["total_items"],
                    "analyzed_items": analysis_result.get("analyzed_count", 0) if analysis_result else 0,
                    "stage_completed": "analysis" if analysis_result else "collection"
                }
            }
            
        except Exception as e:
            app_logger.error(f"å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "pipeline_duration": (datetime.now() - pipeline_start).total_seconds()
            }

    async def analyze_recent_news(self, hours: int = 1, limit: int = 20) -> Dict[str, Any]:
        """åˆ†ææœ€è¿‘çš„æ–°é—»"""
        try:
            app_logger.info(f"å¼€å§‹åˆ†ææœ€è¿‘ {hours} å°æ—¶å†…çš„æ–°é—»...")
            
            # ä½¿ç”¨å®ä½“åˆ†æåè°ƒå™¨è¿›è¡Œåˆ†æ
            result = await entity_analysis_orchestrator.analyze_recent_news(hours=hours, limit=limit)
            
            analyzed_count = result.get('analyzed_count', 0)
            app_logger.info(f"æ–°é—»åˆ†æå®Œæˆ: {analyzed_count} æ¡")
            
            return result
            
        except Exception as e:
            app_logger.error(f"æ–°é—»åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "analyzed_count": 0,
                "error": str(e)
            }

    async def cleanup_old_data(self, days_to_keep: int = 30) -> Dict[str, Any]:
        """æ¸…ç†æ—§æ•°æ®ï¼ˆåŸºäº collected_at å­—æ®µï¼‰ã€‚"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=days_to_keep)

        try:
            db = next(get_db())

            q = db.query(NewsItem).filter(NewsItem.collected_at != None).filter(  # noqa: E711
                NewsItem.collected_at < cutoff_date
            )
            deleted_news = q.count()
            q.delete(synchronize_session=False)
            db.commit()

            app_logger.info(f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_news} æ¡æ—§æ–°é—»")

            return {
                "success": True,
                "deleted_news": deleted_news,
                "cutoff_date": cutoff_date,
            }

        except Exception as e:
            app_logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}


# åˆ›å»ºå…¨å±€å®ä¾‹
data_orchestrator = DataCollectionOrchestrator()
data_collection_orchestrator = data_orchestrator  # ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›åˆ«å
